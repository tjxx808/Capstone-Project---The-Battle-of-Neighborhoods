{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edd5a3d-9f21-4c37-ae6d-22d678b083d4",
   "metadata": {},
   "source": [
    "# Capstone Project -Week 01 - FINAL\n",
    "## IBM Data Science Certificate\n",
    "\n",
    "# 50 Munich Restaurants from Yelp Business Search Database\n",
    "# Data Science Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc2111-8fac-4a68-b96b-a0e876c42f0c",
   "metadata": {},
   "source": [
    "# Week01 -  you will required to submit the following:\n",
    "\n",
    "## 1. A description of the problem and a discussion of the background. (15 marks)\n",
    "\n",
    "This Capstone Project for the IBM Data Science Certificate streams data using the Yelp API - url='https://api.yelp.com/v3/businesses/search' - this collects max. 50 entries for Restaurant Data in Munich, Germany.\n",
    "This Data Set allows us to uniquely categorize the restaurants, name, address, latitude, longitude, review counts, rating and obviously restaurant genres - ie. italian, bavarian, spanisch, tapas, vietnam, sushi, pan-asian, mexican, cocktail bar, bistro, japanese, chinese kebab, etc. \n",
    "There are many Business Search APIs on the web: Google, Foursquare, Yahoo..., I choose Yelp API since it is quite straight forward implementing it into Python v3.6 - Version that is used in this assignment. Choosing the API is quite essential in this assignment since we are collecting Business Data for Munich, Germany, the APIs specify how data is streamed (requst obj, json...) but also what data is collected (Name, Address, coordinates, category, reviews...). Not all APIs offer the same feature set as Yelp. \n",
    "The only limitation to the Yelp API is the maximum number of search entries which are limited to 50.\n",
    "\n",
    "So, for this assignment as mentioned in our lectures, Business Understanding is collecting via Yelp API Restaurants/Bistros data based in Munich, Germany. The analytic approach is to categorize data and later process the database with data science tools to extract information and highlight certain patterns. This analytic approach is applied in data science since it connects business understanding with data requirements and data processing, which is the core of data science since it leads to data visualization, evaluation and deployment. \n",
    "\n",
    "## 2. A description of the data and how it will be used to solve the problem. (15 marks)\n",
    "\n",
    "Following the \"Data Chain\", described in our lecture, we consider the diagram \"From Understanding to Approach\" mentioned in Module 3, we considered Business Understanding and Analytic Approach in the previous section.\n",
    "Now we shift to \"Data Chain\" - Data Requirements/Collection/Understanding/Preparation/Modeling.\n",
    "All of the following code and data processing is included in the Capstone Project - Week 02 . which includes the Jupyter Note Book and Python Source Code for this assignment.\n",
    "\n",
    "Data Requirements comprises finding a way to collect data from the web, store and process it in Python, we do this via the Yelp API, accessing - url='https://api.yelp.com/v3/businesses/search', \n",
    "we specify which parameters we stream - \n",
    "params = {'term':'restaurant','location':'Munich', 'limit':'50'}\n",
    "Data Collection is implemented by, declaring a request object to store the Yelp API request Data -\n",
    "req=requests.get(url, params=params, headers=headers)\n",
    "Data Preparation - the Yelp request is implemented by parsing the request object as text and json data types -\n",
    "JSON request data is often multi nested and quite complex to store in 2-D arrays, therefore a number of tools have been developed to flatten JSON data. Python offers many JSON flattening/normalization tools, which fall into Data Preparation, \n",
    "(data = json.loads(req.text)) - can be flattened since it is multi nested JSON data text type.\n",
    "Since we are dealing with multi nested JSON objects there is also multi flattening which is applied:\n",
    "\n",
    "1. df2 = pd.DataFrame.from_dict(req.json()['businesses']) - using this command we store JSON data in the df2 dataframe - columns coordinates, address and categories are nested\n",
    "2. d1 = pd.json_normalize(data[\"businesses\"]) - we normalize data, d1 contains the normalized json dataframe - the coordinates and address columns are flattened, the categories column is still nested\n",
    "3. pd.DataFrame(d3[\"cat\"].apply(pd.Series)) - is applied to flatten the categories=cat column - from here we can extract the restaurants category type - sushi, vietnamese, pan-asian, tapas, mexican, bavarian, italian...\n",
    "\n",
    "As you can see the Data Preparation stage can be quite complex and it is a significant component of data science, often it entails normalizing and filtering data to make it easier to process with data science tools. \n",
    "\n",
    "Finally, Data Evaluation, Visualization and Deployment are extensively covered in Week 02 of the Capstone Project Assignment.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69eb87-486d-4003-ae16-841f7d84da44",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be1f5c-61a8-48d2-a975-e807c7e24631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2c715-f818-4fae-a31b-d7f1a8bae7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d6fa8-8411-4698-b9f2-4cf0222a205f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ffca68-5653-4162-aa67-fc839b956aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe021c-1558-4256-9c32-06fc241f2ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f09f7f6-2c97-40fd-b625-89ae1c3adfec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
