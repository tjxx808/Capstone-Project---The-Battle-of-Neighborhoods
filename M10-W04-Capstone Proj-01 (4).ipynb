{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a47bfbb-2702-4a63-9614-1528466d5c0c",
   "metadata": {},
   "source": [
    "# Module 10 - Week 04\n",
    "# Capstone Project - The Battle of Neighborhoods\n",
    "\n",
    "# Part 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7382b19-4cb8-4078-9b2a-b777dbf979de",
   "metadata": {},
   "source": [
    "# 1. A description of the problem and a discussion of the background. (15 marks)\n",
    "\n",
    "This assignment compares \"data\" from 2 cities, namely New York and Toronto. The term \"Data\" and its nature will be described later in the next section. \n",
    "First of all both cities should be analyzed using Data Science Python tools presented in this course IBM Data Science - one source of data is Wikipedia with its respective URLs. \n",
    "In this Module 10 - Week 03 we have submitted assignments - Segmenting and Clustering Neighborhoods in New York City / Toronto, which are used for reference. \n",
    "In these assignments, html data was imported, parsed using pandas, beautiful soup (for Toronto) into data frames.\n",
    "Once we store these data frames we can clean, normalize, refine and process data using Python data science tools. \n",
    "Some external web providers/services such as Google and Foursquare are used to import, store, process and visualize data, more on that later.\n",
    "Data related to cities, can be categorized as gastronomy data: restaurants, bars, venues, their names, genre, address and feedback uploaded by customers visiting the venue. Another data category which can be applied to cities is, demographic data, which includes: age groups, sex, marital status, race group, children living at a particular address, job type, income range and various other demographic parameters. These different parameters can all be imported from different sources, mainly web or excel/access documents and stored in data frames. Once this is done, various data science tools can be applied. I chose as data categorizing - gastronomy so food & bar services, comprising mainly restaurants and bars. The previous assignment in Week 03 of this module already uses gastronomy services as category to analyze data.    \n",
    "Initially, data should be cleaned and refined, so empty and erroneous entries deleted.\n",
    "Different approaches can be used to process data, one of them is categorizing.\n",
    "Every entry should be categorized:\n",
    "-Venue type: Restaurant/Bar/Bistro \n",
    "-Genre: Restaurant(vietnamese/italian/japanese/spanish) I Bars (cocktail bar/late night/bistro/Irish pub/Australian Pub)\n",
    "-Address\n",
    "-Longitude\n",
    "-Latitude\n",
    "-Customer Feedback\n",
    " \n",
    "# 2. A description of the data and how it will be used to solve the problem. (15 marks)\n",
    "\n",
    "Initially data needs to be imported, for both cities from the web, there are two approaches\n",
    "\n",
    "1. New York - \n",
    "Here html data is imported into Python using a Link provided by Coursera, stored in a json file, later parsed to a dataframe\n",
    "\n",
    "!wget -q -O 'newyork_data.json' https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/labs/newyork_data.json\n",
    "\n",
    "\n",
    "read json file from https link, parse it into a dataframe \n",
    "\n",
    "with open('newyork_data.json') as json_data:\n",
    "newyork_data = json.load(json_data)\n",
    "\n",
    "later the dataframe is converted into pandas, a helpful data manipulation tool, in order to apply  data science tools. \n",
    "\n",
    "# An empty dataframe is created\n",
    "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
    "\n",
    "# Empty pandas dataframe\n",
    "neighborhoods = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# For loops to populate dataframe with entries from json file\n",
    "\"\"\"\n",
    "for data in neighborhoods_data:\n",
    "    borough = neighborhood_name = data['properties']['borough'] \n",
    "    neighborhood_name = data['properties']['name']\n",
    "        \n",
    "    neighborhood_latlon = data['geometry']['coordinates']\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    neighborhoods = neighborhoods.append({'Borough': borough,\n",
    "                                          'Neighborhood': neighborhood_name,\n",
    "                                          'Latitude': neighborhood_lat,\n",
    "                                          'Longitude': neighborhood_lon}, ignore_index=True)\n",
    "\"\"\"\n",
    "\n",
    "# visualize newly created dataframe\n",
    "neighborhoods.head(8)\n",
    "\n",
    "2. Toronto -\n",
    "For Toronto we import data from a Wikipedia url and the import it into a dataframe using beatifulsoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'\n",
    "r=requests.get(url)\n",
    "soup=BeautifulSoup(r.text,'html5lib')\n",
    "\n",
    "For Toronto data, html data is stored from URL - Wiki page - into a dataframe using beautifulsoup, a web scraping tool to import html/ xml data.\n",
    "\n",
    "Once data is stored in dataframes it can initialized, cleaned and processed.\n",
    "\n",
    "Obviously, the appropriate Python libraries have to be imported to read and process data in Python, below is a list of them\n",
    "\n",
    "\n",
    "!pip install -U numpy\n",
    "!pip install -U pandas\n",
    "!pip install -U scipy\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U imbalanced-learn\n",
    "\n",
    "!pip install lxml\n",
    "import lxml\n",
    "\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import requests \t    # library to handle requests\n",
    "import pandas as pd \t# library for data analysis\n",
    "import numpy as np \t    # library to handle data in a vectorized manner\n",
    "import random \t\t    # library for random number generation\n",
    "\n",
    "!pip install geopy\n",
    "from geopy.geocoders import Nominatim \t# module to convert an address into latitude and longitude values\n",
    "\n",
    "from IPython.display import Image # libraries for displaying images\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from pandas.io.json import json_normalize # tranforming json file into a pandas dataframe library\n",
    "\n",
    "! pip install folium==0.5.0\n",
    "import folium # plotting library\n",
    "\n",
    "!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "Many packages and libraries have to be imported into Python to allow a comprehensive Data Science Analysis. \n",
    "External providers such as Foursquare, Google have their own libraries which need to be imported separately. Tools such as, BeatifulSoup to handle html data, folium for plotting, geopy to work with latitude and longitude, ipython to display images and obviously numpy/pd for panda dataframes, scipi, scikit-learn and data science libraries respectively. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab455103-5da4-448c-97fb-da384e9095b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fe997-9d22-4899-896e-8a903c0b070d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b25f5-2127-4f5d-874b-05cbe961ad52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81485704-3325-437c-a3a6-9ca7cbb90610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c802a-2ed0-4f31-8c93-1173a985e464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00b589-7961-4756-aa2c-292759e5e706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b420b-165f-48f8-a4d7-400a065896f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
